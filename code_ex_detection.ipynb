{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30f3557",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "Run all the cells up to \"Execute Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9d71de-38dd-4844-9a03-b04eaab1ede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\57312\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import openvino as ov\n",
    "from openvino.runtime.ie_api import CompiledModel\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py',\n",
    "    filename='notebook_utils.py'\n",
    ")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4282ef2-4f3e-49cb-8823-71b0ccc71b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = \"model\"\n",
    "# The name of the model from Open Model Zoo.\n",
    "model_name = \"action-recognition-0001\"\n",
    "# Selected precision (FP32, FP16, FP16-INT8).\n",
    "precision = \"FP16\"\n",
    "model_path_decoder = (\n",
    "    f\"model/intel/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.xml\"\n",
    ")\n",
    "model_path_encoder = (\n",
    "    f\"model/intel/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.xml\"\n",
    ")\n",
    "encoder_url = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/temp/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.xml\"\n",
    "decoder_url = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/temp/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.xml\"\n",
    "\n",
    "if not os.path.exists(model_path_decoder):\n",
    "    utils.download_ir_model(decoder_url, Path(model_path_decoder).parent)\n",
    "if not os.path.exists(model_path_encoder):\n",
    "    utils.download_ir_model(encoder_url, Path(model_path_encoder).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243703fd-a3ce-4375-939c-6572acdc6753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data\\kinetics.txt' already exists.\n",
      "['abseiling', 'air drumming', 'answering questions', 'applauding', 'applying cream', 'archery', 'arm wrestling', 'arranging flowers', 'assembling computer'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# Download the text from the openvino_notebooks storage\n",
    "vocab_file_path = utils.download_file(\n",
    "    \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/text/kinetics.txt\",\n",
    "    directory=\"data\"\n",
    ")\n",
    "\n",
    "with vocab_file_path.open(mode='r') as f:\n",
    "    labels = [line.strip() for line in f]\n",
    "\n",
    "print(labels[0:9], np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4432139a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pull ups'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specified_list = ['clean and jerk','throwing ball','swinging legs','stretching leg','squat','situp','side kick',\n",
    "                  'push up','pull ups','snatch weight lifting','lunge','exercising with an exercise ball',\n",
    "                  'exercising arm','deadlifting','yoga','stretching arm']\n",
    "labels = ['no exercise' if  all(spec not in label.lower() for spec in specified_list) else label for label in labels]\n",
    "labels[255] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebac336-20a5-40a5-8720-0ac371e90feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53d954b8a6e4469aec0102eecf5ec41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84dc81a-73ed-44f8-a0e4-2c5617a1496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime.\n",
    "core = ov.Core()\n",
    "\n",
    "\n",
    "def model_init(model_path: str, device: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Read the network and weights from a file, load the\n",
    "    model on CPU and get input and output names of nodes\n",
    "\n",
    "    :param:\n",
    "            model: model architecture path *.xml\n",
    "            device: inference device\n",
    "    :retuns:\n",
    "            compiled_model: Compiled model\n",
    "            input_key: Input node for model\n",
    "            output_key: Output node for model\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the network and corresponding weights from a file.\n",
    "    model = core.read_model(model=model_path)\n",
    "    # Compile the model for specified device.\n",
    "    compiled_model = core.compile_model(model=model, device_name=device)\n",
    "    # Get input and output names of nodes.\n",
    "    input_keys = compiled_model.input(0)\n",
    "    output_keys = compiled_model.output(0)\n",
    "    return input_keys, output_keys, compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e005cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Angles of Joints\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2ef408",
   "metadata": {},
   "outputs": [],
   "source": [
    "joints_dictionary = {'push up': [['LEFT_SHOULDER','LEFT_ELBOW','LEFT_WRIST'], ['RIGHT_SHOULDER','RIGHT_ELBOW','RIGHT_WRIST'],[155, 90]],\n",
    "                     'pull ups': [['LEFT_SHOULDER','LEFT_ELBOW','LEFT_WRIST'], ['RIGHT_SHOULDER','RIGHT_ELBOW','RIGHT_WRIST'],[155, 80]],\n",
    "                     'situp': [['LEFT_SHOULDER','LEFT_HIP','LEFT_ANKLE'], ['RIGHT_SHOULDER','RIGHT_HIP','RIGHT_ANKLE'],[160, 130]],\n",
    "                     'squat': [['LEFT_SHOULDER','LEFT_HIP','LEFT_KNEE'], ['RIGHT_SHOULDER','RIGHT_HIP','RIGHT_KNEE'],[155, 110]],\n",
    "                     'snatch weight lifting': [['LEFT_WRIST','LEFT_SHOULDER','LEFT_ANKLE'], ['RIGHT_WRIST','RIGHT_SHOULDER', 'RIGHT_ANKLE'],[155, 60]],\n",
    "                     'burpee': [['LEFT_HIP','LEFT_KNEE', 'LEFT_ANKLE'], ['RIGHT_HIP','RIGHT_KNEE', 'RIGHT_ANKLE'],[155, 90]]\n",
    "                     }\n",
    "#Returns the 3 angles that constitute a side of the body\n",
    "def initialize_joints(side_list, landmarks):\n",
    "    joint_coordinates = []\n",
    "\n",
    "    for joint_name in side_list:\n",
    "    # Get the index of the joint name in mp_pose.PoseLandmark enum\n",
    "        joint_index = getattr(mp_pose.PoseLandmark, joint_name).value\n",
    "        # Extract the x and y coordinates of the joint and append them to joint_coordinates\n",
    "        joint_x = landmarks[joint_index].x\n",
    "        joint_y = landmarks[joint_index].y\n",
    "        joint_coordinates.append([joint_x, joint_y])\n",
    "\n",
    "    return joint_coordinates[0], joint_coordinates[1], joint_coordinates[2]\n",
    "\n",
    "# Customized exercise finder\n",
    "def print_actual_exercise(exercie_results_df):\n",
    "    label_probabilities = dict(zip(exercie_results_df['label'], exercie_results_df['probability']))\n",
    "\n",
    "    situp = label_probabilities.get('situp', None)\n",
    "    exercising_with_an_exercise_ball = label_probabilities.get('exercising with an exercise ball', None)\n",
    "    throwing_ball = label_probabilities.get('throwing ball', None)\n",
    "    stretching_leg = label_probabilities.get('stretching leg', None)\n",
    "\n",
    "    squat = label_probabilities.get('squat', None)\n",
    "    lunge = label_probabilities.get('lunge', None)\n",
    "    snatch_weight_lifting = label_probabilities.get('snatch weight lifting', None)\n",
    "    clean_and_jerk = label_probabilities.get('clean and jerk', None)\n",
    "\n",
    "    deadlifting = label_probabilities.get('deadlifting', None)\n",
    "    push_up = label_probabilities.get('push up', None)\n",
    "    exercising_arm = label_probabilities.get('exercising arm', None)\n",
    "    swinging_legs = label_probabilities.get('swinging legs', None)\n",
    "\n",
    "    stretching_arm = label_probabilities.get('stretching arm', None)\n",
    "    side_kick = label_probabilities.get('side kick', None)\n",
    "    pull_ups = label_probabilities.get('pull ups', None)\n",
    "    yoga = label_probabilities.get('yoga', None)\n",
    "\n",
    "    current_exercise = 'no exercise'\n",
    "    base_situp = situp + exercising_with_an_exercise_ball + throwing_ball + stretching_leg + yoga\n",
    "    base_squad = squat + lunge + snatch_weight_lifting + situp\n",
    "    base_snatch = snatch_weight_lifting + lunge + clean_and_jerk + deadlifting + squat\n",
    "    base_pushup = push_up + exercising_arm + stretching_leg + swinging_legs + stretching_arm + side_kick + exercising_with_an_exercise_ball\n",
    "    base_burpee = push_up + squat + exercising_arm + situp + lunge + throwing_ball\n",
    "\n",
    "    if pull_ups >= 0.8:\n",
    "        current_exercise = 'pull ups'\n",
    "    #    print(f'Recognized exercie: {current_exercise} with {pull_ups}')\n",
    "    elif situp >= 0.22:\n",
    "        if base_situp >= 0.6:\n",
    "            current_exercise = 'situp'\n",
    "    #        print(f'Recognized exercie: {current_exercise} with {base_situp}')\n",
    "    elif squat >= 0.3:\n",
    "        if base_squad >= 0.6 and snatch_weight_lifting < 0.1:\n",
    "            current_exercise = 'squat'\n",
    "    #        print(f'Recognized exercie: {current_exercise} with {base_squad}')\n",
    "    elif snatch_weight_lifting + clean_and_jerk + deadlifting > 0.15:\n",
    "        if  base_snatch > 0.5:\n",
    "            current_exercise = 'snatch weight lifting'\n",
    "    #        print(f'Recognized exercie: {current_exercise} with {base_snatch}')\n",
    "    elif push_up > 0.35 or stretching_leg > 0.6:\n",
    "        if base_pushup > 0.7:\n",
    "            current_exercise = 'push up'\n",
    "    #        print(f'Recognized exercie: {current_exercise} with {base_pushup}')\n",
    "    elif push_up < 0.25 and squat < 0.25 and situp < 0.25 and  base_burpee > 0.6:\n",
    "        current_exercise = 'burpee'\n",
    "        print(f'Recognized exercie: {current_exercise} with {base_pushup}')\n",
    "    #print(f'Recognized exercie: {current_exercise}')\n",
    "    return current_exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31aaf5a1-fd21-4d82-bed1-ea877e92d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder initialization\n",
    "input_key_en, output_keys_en, compiled_model_en = model_init(model_path_encoder, device.value)\n",
    "# Decoder initialization\n",
    "input_key_de, output_keys_de, compiled_model_de = model_init(model_path_decoder, device.value)\n",
    "\n",
    "# Get input size - Encoder.\n",
    "height_en, width_en = list(input_key_en.shape)[2:]\n",
    "# Get input size - Decoder.\n",
    "frames2decode = list(input_key_de.shape)[0:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "734df139-c3de-48c9-9f13-ec2415baa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center crop squared the original frame to standardize the input image to the encoder model\n",
    "\n",
    "    :param frame: input frame\n",
    "    :returns: center-crop-squared frame\n",
    "    \"\"\"    \n",
    "    img_h, img_w, _ = frame.shape\n",
    "    min_dim = min(img_h, img_w)\n",
    "    start_x = int((img_w - min_dim) / 2.0)\n",
    "    start_y = int((img_h - min_dim) / 2.0)\n",
    "    roi = [start_y, (start_y + min_dim), start_x, (start_x + min_dim)]\n",
    "    return frame[start_y : (start_y + min_dim), start_x : (start_x + min_dim), ...], roi\n",
    "\n",
    "\n",
    "#Will crop and center the image on the identified body.\n",
    "def center_body (frame, thres = 0.05):\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "    landmarks = [[lm.x, lm.y] for lm in landmarks]\n",
    "    landmarks = np.array(landmarks)\n",
    "\n",
    "    #Get Landmark X and Y\n",
    "    x_coords = landmarks[:, 0]\n",
    "    y_coords = landmarks[:, 1]\n",
    "\n",
    "    # Find min and max x and y coordinates\n",
    "    min_x = round(max(np.amin(x_coords) - thres, 0) * frame.shape[1])\n",
    "    min_y = round(max(np.amin(y_coords) - thres, 0) * frame.shape[0])\n",
    "    max_x = round(min(np.amax(x_coords) + thres, 1) * frame.shape[1])\n",
    "    max_y = round(min(np.amax(y_coords) + thres, 1) * frame.shape[0])\n",
    "\n",
    "    # Create width and height.\n",
    "    w = max_x - min_x\n",
    "    h = max_y - min_y\n",
    "\n",
    "    # Determine size of the square frame\n",
    "    size = max(w, h)\n",
    "\n",
    "    # Calculate center of the bounding box\n",
    "    center_x = min_x + w // 2\n",
    "    center_y = min_y + h // 2\n",
    "\n",
    "    # Calculate coordinates for cropping\n",
    "    start_x = max(0, center_x - size // 2)\n",
    "    start_y = max(0, center_y - size // 2)\n",
    "    end_x = min(frame.shape[1], start_x + size)\n",
    "    end_y = min(frame.shape[0], start_y + size)\n",
    "\n",
    "    roi = [start_y, (start_y + end_y), start_x, (start_x + end_x)]\n",
    "    cropped_image = frame[start_y:end_y, start_x:end_x]\n",
    "    return cropped_image, roi\n",
    "\n",
    "\n",
    "def adaptive_resize(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "     The frame going to be resized to have a height of size or a width of size\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized frame, np.array type\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    scale = size / min(h, w)\n",
    "    w_scaled, h_scaled = int(w * scale), int(h * scale)\n",
    "    if w_scaled == w and h_scaled == h:\n",
    "        return frame\n",
    "    #return cv2.resize(frame, (w_scaled, h_scaled))\n",
    "    return cv2.resize(frame, (size, size))\n",
    "\n",
    "\n",
    "def decode_output(probs: np.ndarray, labels: np.ndarray, top_k: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decodes top probabilities into corresponding label names\n",
    "\n",
    "    :param probs: confidence vector for 400 actions\n",
    "    :param labels: list of actions\n",
    "    :param top_k: The k most probable positions in the list of labels\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "\n",
    "    top_ind = np.argsort(-1 * probs)[:top_k]\n",
    "    out_label = np.array(labels)[top_ind.astype(int)]\n",
    "    decoded_labels = [out_label[0][0], out_label[0][1], out_label[0][2]]\n",
    "    top_probs = np.array(probs)[0][top_ind.astype(int)]\n",
    "    decoded_top_probs = [top_probs[0][0], top_probs[0][1], top_probs[0][2]]\n",
    "\n",
    "    \"\"\"\n",
    "    # Step 1: Create a DataFrame with columns 'label' and 'probability'\n",
    "    df = pd.DataFrame({'label': labels, 'probability': probs[0]})\n",
    "    # Step 3: Group by 'label' with the sum of probabilities\n",
    "    grouped_df = df.groupby('label')['probability'].sum().reset_index()\n",
    "    current_exercise = print_actual_exercise(grouped_df)\n",
    "\n",
    "    # Step 4: Get the top k results\n",
    "    sorted_df = grouped_df.sort_values(by='probability', ascending=False).head(top_k)\n",
    "    \n",
    "    # Get decoded labels and probabilities\n",
    "    decoded_labels = sorted_df['label'].tolist()\n",
    "    decoded_top_probs = sorted_df['probability'].tolist()\n",
    "\n",
    "    return decoded_labels, decoded_top_probs, current_exercise\n",
    "\n",
    "\n",
    "def rec_frame_display(frame: np.ndarray, roi) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw a rec frame over actual frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param roi: Region of interest, image section processed by the Encoder\n",
    "    :returns: frame with drawed shape\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "    # Write ROI over actual frame\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    org = (roi[2] + 3, roi[1] - 3)\n",
    "    org2 = (roi[2] + 2, roi[1] - 2)\n",
    "    FONT_SIZE = 0.5\n",
    "    FONT_COLOR = (0, 200, 0)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    cv2.putText(frame, \"ROI\", org2, FONT_STYLE, FONT_SIZE, FONT_COLOR2)\n",
    "    cv2.putText(frame, \"ROI\", org, FONT_STYLE, FONT_SIZE, FONT_COLOR)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def display_text_fnc(frame: np.ndarray, display_text: str, index: int):\n",
    "    \"\"\"\n",
    "    Include a text on the analyzed frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param display_text: text to add on the frame\n",
    "    :param index: index line dor adding text\n",
    "\n",
    "    \"\"\"\n",
    "    # Configuration for displaying images with text.\n",
    "    FONT_COLOR = (0, 255, 0)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_DUPLEX\n",
    "    FONT_SIZE = 1\n",
    "    TEXT_VERTICAL_INTERVAL = 25\n",
    "    TEXT_LEFT_MARGIN = 15\n",
    "\n",
    "    text_loc = (TEXT_LEFT_MARGIN, TEXT_VERTICAL_INTERVAL * (index + 1))\n",
    "    text_loc2 = (TEXT_LEFT_MARGIN + 1, TEXT_VERTICAL_INTERVAL * (index + 1) + 1)\n",
    "    cv2.putText(frame, display_text, text_loc2, FONT_STYLE, FONT_SIZE, FONT_COLOR2, 2)\n",
    "    cv2.putText(frame, display_text, text_loc, FONT_STYLE, FONT_SIZE, FONT_COLOR, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67522f94-596e-4c77-8e3e-c6e252727b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preparing frame before Encoder.\n",
    "    The image should be scaled to its shortest dimension at \"size\"\n",
    "    and cropped, centered, and squared so that both width and\n",
    "    height have lengths \"size\". The frame must be transposed from\n",
    "    Height-Width-Channels (HWC) to Channels-Height-Width (CHW).\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized and cropped frame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        (preprocessed, roi) = center_body(frame)\n",
    "        preprocessed = adaptive_resize(preprocessed, size)\n",
    "\n",
    "    except:\n",
    "        # Adaptative resize\n",
    "        preprocessed = adaptive_resize(frame, size)\n",
    "        # Center_crop\n",
    "        (preprocessed, roi) = center_crop(preprocessed)\n",
    "        \n",
    "    # Transpose frame HWC -> CHW\n",
    "    preprocessed = preprocessed.transpose((2, 0, 1))[None,]  # HWC -> CHW\n",
    "    return preprocessed, roi\n",
    "\n",
    "\n",
    "def encoder(\n",
    "    preprocessed: np.ndarray,\n",
    "    compiled_model: CompiledModel\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Encoder Inference per frame. This function calls the network previously\n",
    "    configured for the encoder model (compiled_model), extracts the data\n",
    "    from the output node, and appends it in an array to be used by the decoder.\n",
    "\n",
    "    :param: preprocessed: preprocessing frame\n",
    "    :param: compiled_model: Encoder model network\n",
    "    :returns: encoder_output: embedding layer that is appended with each arriving frame\n",
    "    \"\"\"\n",
    "    output_key_en = compiled_model.output(0)\n",
    "\n",
    "    # Get results on action-recognition-0001-encoder model\n",
    "    infer_result_encoder = compiled_model([preprocessed])[output_key_en]\n",
    "    return infer_result_encoder\n",
    "\n",
    "\n",
    "def decoder(encoder_output: List, compiled_model_de: CompiledModel) -> List:\n",
    "    \"\"\"\n",
    "    Decoder inference per set of frames. This function concatenates the embedding layer\n",
    "    froms the encoder output, transpose the array to match with the decoder input size.\n",
    "    Calls the network previously configured for the decoder model (compiled_model_de), extracts\n",
    "    the logits and normalize those to get confidence values along specified axis.\n",
    "    Decodes top probabilities into corresponding label names\n",
    "\n",
    "    :param: encoder_output: embedding layer for 16 frames\n",
    "    :param: compiled_model_de: Decoder model network\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "    \"\"\"\n",
    "    # Concatenate sample_duration frames in just one array\n",
    "    decoder_input = np.concatenate(encoder_output, axis=0)\n",
    "    # Organize input shape vector to the Decoder (shape: [1x16x512]]\n",
    "    decoder_input = decoder_input.transpose((2, 0, 1, 3))\n",
    "    decoder_input = np.squeeze(decoder_input, axis=3)\n",
    "    output_key_de = compiled_model_de.output(0)\n",
    "    # Get results on action-recognition-0001-decoder model\n",
    "    result_de = compiled_model_de([decoder_input])[output_key_de]\n",
    "    # Normalize logits to get confidence values along specified axis\n",
    "    probs = softmax(result_de - np.max(result_de))\n",
    "    df = pd.DataFrame({'label': labels, 'probability': probs[0]})\n",
    "    grouped_df = df.groupby('label')['probability'].sum().reset_index()\n",
    "    sorted_df = grouped_df.sort_values(by='probability', ascending=False)\n",
    "\n",
    "    # Decodes top probabilities into corresponding label names\n",
    "    decoded_labels, decoded_top_probs, current_exercise = decode_output(probs, labels, top_k=3)\n",
    "    return decoded_labels, decoded_top_probs, current_exercise\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes logits to get confidence values along specified axis\n",
    "    x: np.array, axis=None\n",
    "    \"\"\"\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9dde48b-ecbe-4691-9616-a289a93a4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_action_recognition(\n",
    "    source: str = \"0\",\n",
    "    flip: bool = True,\n",
    "    use_popup: bool = False,\n",
    "    compiled_model_en: CompiledModel = compiled_model_en,\n",
    "    compiled_model_de: CompiledModel = compiled_model_de,\n",
    "    skip_first_frames: int = 0,\n",
    "):\n",
    "    size = height_en  # Encoder requiered size\n",
    "    sample_duration = frames2decode  # Number of frames that decoder needs\n",
    "    # Select FPS source.\n",
    "    fps = 16\n",
    "    player = None\n",
    "    exercise_dict = {} #Store repetitions per exercise on the video\n",
    "    record_video = {'no exercise':[]}\n",
    "\n",
    "    #Mediapipe Pose Detection\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    \n",
    "        try:\n",
    "            # Create a video player.\n",
    "            player = utils.VideoPlayer(source, flip=flip, fps=fps, skip_first_frames=skip_first_frames)\n",
    "            # Start capturing.\n",
    "            player.start()\n",
    "            if use_popup:\n",
    "                title = \"Press ESC to Exit\"\n",
    "                cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "            processing_times = collections.deque()\n",
    "            processing_time = 0\n",
    "            encoder_output = []\n",
    "            decoded_labels = ['no exercise', 0, 0]\n",
    "            actual_exercise = 'no exercise'\n",
    "            decoded_top_probs = [0, 0, 0]\n",
    "            counter = 0\n",
    "            stage = None    #Stage inside a Cycle. Extension or Contraction\n",
    "\n",
    "\n",
    "            # Create a text template to show inference results over video.\n",
    "            text_inference_template = \"Infer Time:{Time:.1f}ms,{fps:.1f}FPS\"\n",
    "            text_template = \"{label},{conf:.2f}%\"\n",
    "\n",
    "            while True:\n",
    "                counter = counter + 1\n",
    "\n",
    "                # Read a frame from the video stream.\n",
    "                frame = player.next()\n",
    "                if frame is None:\n",
    "                    print(\"Source ended\")\n",
    "                    break\n",
    "\n",
    "                scale = 1280 / max(frame.shape)\n",
    "\n",
    "\n",
    "                ####### Define Current Exercise Probabilities\n",
    "                if counter % 2 == 0:\n",
    "                    # Preprocess frame before Encoder.\n",
    "                    (preprocessed, _) = preprocessing(frame, size)\n",
    "                    #record_video['no exercise'].append(preprocessed)\n",
    "\n",
    "                    # Measure processing time.\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Encoder Inference per frame\n",
    "                    encoder_output.append(encoder(preprocessed, compiled_model_en))\n",
    "\n",
    "                    # Decoder inference per set of frames\n",
    "                    # Wait for sample duration to work with decoder model.\n",
    "                    if len(encoder_output) == sample_duration:\n",
    "                        decoded_labels, decoded_top_probs, actual_exercise = decoder(encoder_output, compiled_model_de)\n",
    "                        encoder_output = []\n",
    "\n",
    "                        if actual_exercise != 'no exercise':\n",
    "                            print(actual_exercise)\n",
    "\n",
    "                    # Inference has finished. Display the results.\n",
    "                    stop_time = time.time()\n",
    "\n",
    "                    # Calculate processing time.\n",
    "                    processing_times.append(stop_time - start_time)\n",
    "\n",
    "                    # Use processing times from last 200 frames.\n",
    "                    if len(processing_times) > 200:\n",
    "                        processing_times.popleft()\n",
    "\n",
    "                    # Mean processing time [ms]\n",
    "                    processing_time = np.mean(processing_times) * 1000\n",
    "                    fps = 1000 / processing_time\n",
    "\n",
    "                ####### Define exercise repetitions \n",
    "                if actual_exercise == 'no exercise':\n",
    "                        count_ext = 0\n",
    "                        count_con = 0\n",
    "\n",
    "                elif actual_exercise != 'no exercise':\n",
    "                    # Recolor image to RGB\n",
    "                    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "                \n",
    "                    # Make detection\n",
    "                    results = pose.process(image)\n",
    "                \n",
    "                    # Recolor back to BGR\n",
    "                    image.flags.writeable = True\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                    try:\n",
    "                        landmarks = results.pose_landmarks.landmark\n",
    "                        \n",
    "                        # Get coordinates\n",
    "                        A,B,C = initialize_joints(joints_dictionary[actual_exercise][0],landmarks)\n",
    "                        D,E,F = initialize_joints(joints_dictionary[actual_exercise][1],landmarks)\n",
    "                        \n",
    "                        # Calculate angle\n",
    "                        angle_l = calculate_angle(A, B, C)\n",
    "                        angle_r = calculate_angle(D, E, F)\n",
    "                        #print(f'angle izq: {angle_l}  -  angle der: {angle_r}')\n",
    "\n",
    "                        AD = [(A[0] + D[0]) / 2, (A[1] + D[1]) / 2]\n",
    "                        BE = [(B[0] + E[0]) / 2, (B[1] + E[1]) / 2]\n",
    "                        CF = [(C[0] + F[0]) / 2, (C[1] + F[1]) / 2]\n",
    "                        angle_mid = calculate_angle(AD, BE, CF)\n",
    "                        #print(f'angle izq: {angle_l}  -  angle der: {angle_r}  -  angle_mid: {angle_mid}')\n",
    "\n",
    "                        \n",
    "                        #Special Case for Snatch (One Arm snatch)\n",
    "                        if actual_exercise == 'snatch weight lifting':\n",
    "                        \n",
    "                            #Extension\n",
    "                            if angle_mid > joints_dictionary[actual_exercise][2][0] and stage != \"extension\":\n",
    "                                stage = \"extension\"\n",
    "                                count_ext = 1\n",
    "                                #print(f'stage: {stage}  -  angle mid: {angle_mid}')\n",
    "                            \n",
    "                            #Contraction\n",
    "                            if angle_mid < joints_dictionary[actual_exercise][2][1] and stage !='contraction':\n",
    "                                stage=\"contraction\"\n",
    "                                count_con = 1\n",
    "                                #print(f'stage: {stage}  -  angle mid: {angle_mid}')\n",
    "\n",
    "                        #Special Case for Situp (MiddlePoint)\n",
    "                        elif actual_exercise == 'situp':\n",
    "                        \n",
    "                            #Extension\n",
    "                            if angle_mid > joints_dictionary[actual_exercise][2][0] and stage != \"extension\":\n",
    "                                stage = \"extension\"\n",
    "                                count_ext = 1\n",
    "                                #print(f'stage: {stage}  -  angle mid: {angle_mid}')\n",
    "                            \n",
    "                            #Contraction\n",
    "                            if angle_mid < joints_dictionary[actual_exercise][2][1] and stage !='contraction':\n",
    "                                stage=\"contraction\"\n",
    "                                count_con = 1\n",
    "                                #print(f'stage: {stage}  -  angle mid: {angle_mid}')\n",
    "                        \n",
    "                        else:\n",
    "\n",
    "                            #Extension\n",
    "                            if angle_l > joints_dictionary[actual_exercise][2][0] and angle_r > joints_dictionary[actual_exercise][2][0] and stage != \"extension\":\n",
    "                                stage = \"extension\"\n",
    "                                count_ext = 1\n",
    "                                #print(f'stage: {stage}  -  angulo izq: {angle_l}  -  angulo der: {angle_r}')\n",
    "                            \n",
    "                            #Contraction\n",
    "                            if angle_l < joints_dictionary[actual_exercise][2][1] and angle_r < joints_dictionary[actual_exercise][2][1] and stage !='contraction':\n",
    "                                stage=\"contraction\"\n",
    "                                count_con = 1\n",
    "                                #print(f'stage: {stage}  -  angulo izq: {angle_l}  -  angulo der: {angle_r}')\n",
    "\n",
    "                        #Complete Cycle, Add 1 To counter\n",
    "                        if count_ext + count_con == 2:\n",
    "                            count_ext = 0 \n",
    "                            count_con = 0\n",
    "                            if actual_exercise not in exercise_dict:\n",
    "                                exercise_dict[actual_exercise] = 1\n",
    "                            else:\n",
    "                                exercise_dict[actual_exercise] += 1\n",
    "                            print(exercise_dict)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Adaptative resize for visualization.\n",
    "                if scale < 1:\n",
    "                    frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Results for the customized results\n",
    "                display_text_fnc(frame, f'Current Exercise: {actual_exercise}', 0)\n",
    "                display_text_fnc(frame, 'REPETITIONS:', 1)\n",
    "                \n",
    "                print_count = 0\n",
    "                for exer , reps in exercise_dict.items():\n",
    "                    display_text_fnc(frame, f'{exer} : {reps}', print_count + 2)\n",
    "                    print_count += 1\n",
    "               \n",
    "                record_video['no exercise'].append(frame)\n",
    "                \n",
    "\n",
    "                if use_popup:\n",
    "                    cv2.imshow(title, frame)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    # escape = 27\n",
    "                    if key == 27:\n",
    "                        break\n",
    "                else:\n",
    "                    _, encoded_img = cv2.imencode(\".jpg\", frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "                    i = display.Image(data=encoded_img)\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(i)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            if player is not None:\n",
    "                # Stop capturing.\n",
    "                player.stop()\n",
    "            if use_popup:\n",
    "                cv2.destroyAllWindows()\n",
    "    return exercise_dict, record_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute Model\n",
    "- USE_WEBCAM = False - to analyze a video in 'video_file'.\n",
    "- USE_WEBCAM = True - to analyze webcam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15671edc-cc6e-4fc2-a83a-f867f4e585b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pull ups\n",
      "{'pull ups': 1}\n",
      "{'pull ups': 2}\n",
      "pull ups\n",
      "{'pull ups': 3}\n",
      "{'pull ups': 4}\n",
      "pull ups\n"
     ]
    }
   ],
   "source": [
    "USE_WEBCAM = True\n",
    "\n",
    "cam_id = 0\n",
    "video_file = \"video_path.mp4\"\n",
    "\n",
    "source = cam_id if USE_WEBCAM else video_file\n",
    "additional_options = {\"skip_first_frames\": 0, \"flip\": False} if not USE_WEBCAM else {\"flip\": True}\n",
    "exercise_dict, record = run_action_recognition(source=source, use_popup=True, **additional_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7843a0f",
   "metadata": {},
   "source": [
    "## 3. Record video\n",
    "Optional to create a video of the analyzed frames in the path output_video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca437d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created successfully: recorded.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "fps = 16  # Adjust the frame rate as needed\n",
    "output_video_path = \"recorded.mp4\"\n",
    "#wat, channels, height, width = record['no exercise'][0].shape\n",
    "height, width, channels = record['no exercise'][0].shape\n",
    "video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "for frame in record['no exercise']:\n",
    "    # Ensure the frame data type is uint8\n",
    "    #frame = frame.reshape(3, 224, 224).transpose(1, 2, 0).astype('uint8')\n",
    "    # Write the frame to the video\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "\n",
    "print(\"Video created successfully:\", output_video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
